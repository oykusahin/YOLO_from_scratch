{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO_on_my_own.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S42tfe51x2EA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install tensorflow-gpu==2.0.0\n",
        "! pip install --upgrade keras\n",
        "! pip install opencv-python\n",
        "! pip install pillow Cython lxml jupyter matplotlib\n",
        "! pip install -q tensorflow tensorflow-datasets matplotlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKLey4_KtCDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "from numpy import expand_dims\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "import struct\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Input\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import ZeroPadding2D\n",
        "from keras.layers import UpSampling2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "\n",
        "import numpy as np\n",
        "from numpy import expand_dims\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chl7rjXR3heJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classes(classes_path): \n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names\n",
        "\n",
        "def get_anchors(anchors_path):\n",
        "    with open(anchors_path) as f:\n",
        "        anchors = f.readline()\n",
        "    anchors = [float(x) for x in anchors.split(',')]\n",
        "    return np.array(anchors).reshape(-1, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqbqJ3nH-c-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes_path = '/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/voc_classes.txt'\n",
        "anchors_path = '/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/tiny_yolo_anchors.txt'\n",
        "class_names = get_classes(classes_path)\n",
        "num_classes = len(class_names)\n",
        "anchors = get_anchors(anchors_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zoe7b7ZCFrJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from os import getcwd\n",
        "import glob, os\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "sets=[('2007', 'train'), ('2007', 'val'), ('2007', 'test')]\n",
        "classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
        "\n",
        "def convert_annotation(year, image_id):\n",
        "    file_dir = \"/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/Annotations/\" + str(year) + \"_\" +str(image_id) + \".xml\"\n",
        "    in_file = open(file_dir, \"r+\")\n",
        "    tree=ET.parse(in_file)\n",
        "    root = tree.getroot()\n",
        "    for obj in root.iter('object'):\n",
        "        cls = obj.find('name').text\n",
        "        if cls not in classes:\n",
        "            continue\n",
        "        cls_id = classes.index(cls)\n",
        "        cls_id= np.append(cls_id, 20)\n",
        "        labels = to_categorical(cls_id)\n",
        "        temp_labels=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        for k in range(len(labels)):\n",
        "          temp_labels= temp_labels+ labels[k]\n",
        "        xmlbox = obj.find('bndbox')\n",
        "        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n",
        "        temp_array=[]\n",
        "        temp_array= np.append(temp_array, 1)\n",
        "        temp_array= np.append(temp_array, b) \n",
        "        temp_array = np.append(temp_array, temp_labels[0:20])\n",
        "        return  temp_array\n",
        "\n",
        "file_dir = \"/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/ImageSets/Main/bird_train.txt\"\n",
        "f = open(file_dir, \"r+\")\n",
        "lines= f.readlines()\n",
        "count=0\n",
        "for line in lines:\n",
        "  count+=1\n",
        "  line = line.split(\"_\")\n",
        "  year = line[0]\n",
        "  image_id= line[1].split(\" \")[0]\n",
        "  if count < 11: \n",
        "    y_train = np.append(y_train,convert_annotation(year, image_id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvyTyY-Dht8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np.reshape(y_train,(1,10,25,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuO24BJIkLRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "X_train = []\n",
        "def convert_image(year, image_id):\n",
        "  file_dir = \"/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/JPEGImages/\" + str(year) + \"_\" + str(image_id)+ \".jpg\"\n",
        "  im = Image.open(file_dir, \"r\")\n",
        "  temp_data = np.asarray(im.getdata())\n",
        "  pix_val = np.resize(temp_data,(416, 416, 3))\n",
        "  #pix_val=tf.image.resize_with_crop_or_pad(temp_data, 448, 448)\n",
        "  return pix_val\n",
        "\n",
        "file_dir = \"/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/ImageSets/Main/bird_train.txt\"\n",
        "f = open(file_dir, \"r+\")\n",
        "lines= f.readlines()\n",
        "count=0\n",
        "for line in lines:\n",
        "  count+=1\n",
        "  line = line.split(\"_\")\n",
        "  year = line[0]\n",
        "  image_id= line[1].split(\" \")[0]\n",
        "  if count < 11: \n",
        "   X_train = np.append(X_train,convert_image(year, image_id))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YgvepgayFK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train= np.reshape(X_train,(10,416,416,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScSlh7yW2XOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_cfg(cfgfile):\n",
        "    file = open(cfgfile, 'r')\n",
        "    lines = file.read().split('\\n')                        \n",
        "    lines = [x for x in lines if len(x) > 0]               \n",
        "    lines = [x for x in lines if x[0] != '#']              \n",
        "    lines = [x.rstrip().lstrip() for x in lines]           \n",
        "\n",
        "    block = {}\n",
        "    blocks = []\n",
        "\n",
        "    for line in lines:\n",
        "      if line[0] == \"[\":               \n",
        "        if len(block) != 0:         \n",
        "          blocks.append(block)     \n",
        "          block = {}               \n",
        "        block[\"type\"] = line[1:-1].rstrip()    \n",
        "      else:\n",
        "        key,value = line.split(\"=\")\n",
        "        block[key.rstrip()] = value.lstrip()\n",
        "    blocks.append(block)\n",
        "    return blocks\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ3yuzUq-Ljg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename= \"/content/gdrive/My Drive/Colab Notebooks/sample_data/darknet53.cfg.txt\"\n",
        "blocks = parse_cfg(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLgH7pMQ-_Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "class EmptyLayer(Layer):\n",
        "    def __init__(self):\n",
        "        super(EmptyLayer, self).__init__()\n",
        "\n",
        "class DetectionLayer(Layer):\n",
        "    def __init__(self, anchors):\n",
        "        super(DetectionLayer, self).__init__()\n",
        "        self.anchors = anchors\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz4IqL7K3Xbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_modules(blocks, model):\n",
        "    net_info = blocks[0]         \n",
        "    prev_filters = 3\n",
        "    output_filters = []\n",
        "    W=608\n",
        "\n",
        "    for index, x in enumerate(blocks[1:]):\n",
        "      if (x[\"type\"] == \"convolutional\"):\n",
        "        activation = x[\"activation\"]\n",
        "        try:\n",
        "          batch_normalize = int(x[\"batch_normalize\"])\n",
        "          bias = False\n",
        "        except:\n",
        "          batch_normalize = 0\n",
        "          bias = True\n",
        "\n",
        "        filters= int(x[\"filters\"])\n",
        "        padding = int(x[\"pad\"])\n",
        "        kernel_size = int(x[\"size\"])\n",
        "        strides = int(x[\"stride\"])\n",
        "\n",
        "        if padding:\n",
        "          pad = (kernel_size - 1) // 2\n",
        "          padding = \"same\"\n",
        "        else:\n",
        "          pad = 0\n",
        "          padding = \"valid\"\n",
        "        \n",
        "        #Add the convolutional layer\n",
        "        model.add(Conv2D(input_shape = (416,416,3), \n",
        "                         filters = filters, \n",
        "                         kernel_size = kernel_size, \n",
        "                         strides = strides, \n",
        "                         padding = 'same', \n",
        "                         data_format='channels_last')\n",
        "                 )\n",
        "        \n",
        "        #Add the Batch Norm Layer\n",
        "        if batch_normalize:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "            #Check the activation. \n",
        "            #It is either Linear or a Leaky ReLU for YOLO\n",
        "        if activation == \"leaky\":\n",
        "          model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "        if activation == \"linear\":\n",
        "          model.add(Activation('linear'))\n",
        "\n",
        "        #If it's an upsampling layer\n",
        "        #We use Bilinear2dUpsampling\n",
        "      elif (x[\"type\"] == \"upsample\"):\n",
        "        stride = int(x[\"stride\"])\n",
        "        model.add(UpSampling2D(size=2,interpolation=\"bilinear\"))\n",
        "       \n",
        "       #If it is a route layer\n",
        "      elif (x[\"type\"] == \"route\"):\n",
        "\n",
        "        #Start  of a route\n",
        "        start = int(x[\"layers\"][1])\n",
        "        #end, if there exists one.\n",
        "        try:\n",
        "          end = int(x[\"layers\"][3])\n",
        "        except:\n",
        "          end = 0\n",
        "        \n",
        "        model.add(EmptyLayer())\n",
        "        if end < 0:\n",
        "          filters =output_filters[index - start] + output_filters[index - end]\n",
        "        else:\n",
        "          filters= output_filters[index - start]\n",
        "\n",
        "      #shortcut corresponds to skip connection\n",
        "      elif x[\"type\"] == \"shortcut\":\n",
        "        model.add(EmptyLayer())\n",
        "      \n",
        "      elif x[\"type\"] == \"maxpool\":\n",
        "        size= int(x[\"size\"])\n",
        "        stride = int(x[\"stride\"])\n",
        "        model.add(MaxPooling2D(pool_size=(size, size), strides=(stride, stride), padding='same'))\n",
        "      \n",
        "      elif x[\"type\"] == \"yolo\":\n",
        "        print(\"yolo: \" + str(index))\n",
        "        mask = x[\"mask\"].split(\",\")\n",
        "        mask = [int(x) for x in mask]\n",
        "\n",
        "        anchors = x[\"anchors\"].split(\",\")\n",
        "        anchors = [int(a) for a in anchors]\n",
        "        anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
        "        anchors = [anchors[i] for i in mask]\n",
        "\n",
        "        model.add(DetectionLayer(anchors))\n",
        "\n",
        "      output_filters.append(filters)\n",
        "\n",
        "    return net_info, model, output_filters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afmcqa_l-CCi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2996b2e4-4b29-4983-b67f-2368e393c1d8"
      },
      "source": [
        "model = Sequential()\n",
        "net_info, model, output_filters = create_modules(blocks,model)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yolo: 16\n",
            "yolo: 23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoCGh3teEsGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "960a2663-b017-485f-ae40-8f54e4d26457"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_127 (Conv2D)          (None, 416, 416, 16)      448       \n",
            "_________________________________________________________________\n",
            "batch_normalization_107 (Bat (None, 416, 416, 16)      64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_107 (LeakyReLU)  (None, 416, 416, 16)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_61 (MaxPooling (None, 208, 208, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_128 (Conv2D)          (None, 208, 208, 32)      4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_108 (Bat (None, 208, 208, 32)      128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_108 (LeakyReLU)  (None, 208, 208, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_62 (MaxPooling (None, 104, 104, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_129 (Conv2D)          (None, 104, 104, 64)      18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_109 (Bat (None, 104, 104, 64)      256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_109 (LeakyReLU)  (None, 104, 104, 64)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_63 (MaxPooling (None, 52, 52, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_130 (Conv2D)          (None, 52, 52, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_110 (Bat (None, 52, 52, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_110 (LeakyReLU)  (None, 52, 52, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_64 (MaxPooling (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_131 (Conv2D)          (None, 26, 26, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_111 (Bat (None, 26, 26, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_111 (LeakyReLU)  (None, 26, 26, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_65 (MaxPooling (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_132 (Conv2D)          (None, 13, 13, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_112 (Bat (None, 13, 13, 512)       2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_112 (LeakyReLU)  (None, 13, 13, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_66 (MaxPooling (None, 13, 13, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_133 (Conv2D)          (None, 13, 13, 1024)      4719616   \n",
            "_________________________________________________________________\n",
            "batch_normalization_113 (Bat (None, 13, 13, 1024)      4096      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_113 (LeakyReLU)  (None, 13, 13, 1024)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_134 (Conv2D)          (None, 13, 13, 256)       262400    \n",
            "_________________________________________________________________\n",
            "batch_normalization_114 (Bat (None, 13, 13, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_114 (LeakyReLU)  (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_135 (Conv2D)          (None, 13, 13, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_115 (Bat (None, 13, 13, 512)       2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_115 (LeakyReLU)  (None, 13, 13, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_136 (Conv2D)          (None, 13, 13, 255)       130815    \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 13, 13, 255)       0         \n",
            "_________________________________________________________________\n",
            "detection_layer_19 (Detectio (None, 13, 13, 255)       0         \n",
            "_________________________________________________________________\n",
            "empty_layer_19 (EmptyLayer)  (None, 13, 13, 255)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_137 (Conv2D)          (None, 13, 13, 128)       32768     \n",
            "_________________________________________________________________\n",
            "batch_normalization_116 (Bat (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_116 (LeakyReLU)  (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_9 (UpSampling2 (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "empty_layer_20 (EmptyLayer)  (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_138 (Conv2D)          (None, 26, 26, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_117 (Bat (None, 26, 26, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_117 (LeakyReLU)  (None, 26, 26, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_139 (Conv2D)          (None, 26, 26, 255)       65535     \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 26, 26, 255)       0         \n",
            "_________________________________________________________________\n",
            "detection_layer_20 (Detectio (None, 26, 26, 255)       0         \n",
            "=================================================================\n",
            "Total params: 8,271,966\n",
            "Trainable params: 8,265,598\n",
            "Non-trainable params: 6,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTuEDXIOOK5p",
        "colab_type": "code",
        "outputId": "2fc81b5f-f6a3-44d4-f6d4-980c4b91c0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-163-d768f88d541e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected detection_layer_20 to have shape (26, 26, 255) but got array with shape (10, 25, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1n4iWNw0JWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Darknet(Model):\n",
        "    def __init__(self, filename):\n",
        "        super(Darknet, self).__init__()\n",
        "        self.blocks = parse_cfg(filename)\n",
        "        self.net_info, self.module_list = create_modules(self.blocks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg2BDqcL0Mwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(model, x):\n",
        "    blocksTmp = blocks[1:]\n",
        "    outputs = {}   #We cache the outputs for the route layer\n",
        "    \n",
        "    write = 0     \n",
        "    for i,x in enumerate(blocks[1:]):        \n",
        "        model_type = x[\"type\"]\n",
        "        \n",
        "        if model_type == \"convolutional\" or model_type == \"upsample\":\n",
        "          print(\"ConvolutÅŸonal\")\n",
        "          #x = model.get_layer[i].input.shape\n",
        "        \n",
        "        elif model_type == \"route\":\n",
        "          print(blocksTmp[i][\"layers\"])\n",
        "          layers = blocksTmp[i][\"layers\"][1]\n",
        "          layers = [int(a) for a in layers]\n",
        "\n",
        "          if (layers[0]) > 0:\n",
        "            layers[0] = layers[0] - i\n",
        "\n",
        "          if len(layers) == 1:\n",
        "            x = outputs[i + (layers[0])]\n",
        "          else:\n",
        "            if (layers[1]) > 0:\n",
        "              layers[1] = layers[1] - i\n",
        "\n",
        "              map1 = outputs[i + layers[0]]\n",
        "              map2 = outputs[i + layers[1]]\n",
        "\n",
        "              x = torch.cat((map1, map2), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0R3VwEqXy76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
        "\n",
        "def compose(*funcs):\n",
        "    \"\"\"Compose arbitrarily many functions, evaluated left to right.\n",
        "    Reference: https://mathieularose.com/function-composition-in-python/\n",
        "    \"\"\"\n",
        "    # return lambda x: reduce(lambda v, f: f(v), funcs, x)\n",
        "    if funcs:\n",
        "        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n",
        "    else:\n",
        "        raise ValueError('Composition of empty sequence not supported.')\n",
        "\n",
        "def letterbox_image(image, size):\n",
        "    '''resize image with unchanged aspect ratio using padding'''\n",
        "    iw, ih = image.size\n",
        "    w, h = size\n",
        "    scale = min(w/iw, h/ih)\n",
        "    nw = int(iw*scale)\n",
        "    nh = int(ih*scale)\n",
        "\n",
        "    image = image.resize((nw,nh), Image.BICUBIC)\n",
        "    new_image = Image.new('RGB', size, (128,128,128))\n",
        "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
        "    return new_image\n",
        "\n",
        "def rand(a=0, b=1):\n",
        "    return np.random.rand()*(b-a) + a\n",
        "\n",
        "def get_random_data(annotation_line, input_shape, random=True, max_boxes=20, jitter=.3, hue=.1, sat=1.5, val=1.5, proc_img=True):\n",
        "    '''random preprocessing for real-time data augmentation'''\n",
        "    line = annotation_line.split()\n",
        "    image = Image.open(line[0])\n",
        "    iw, ih = image.size\n",
        "    h, w = input_shape\n",
        "    box = np.array([np.array(list(map(int,box.split(',')))) for box in line[1:]])\n",
        "\n",
        "    if not random:\n",
        "        # resize image\n",
        "        scale = min(w/iw, h/ih)\n",
        "        nw = int(iw*scale)\n",
        "        nh = int(ih*scale)\n",
        "        dx = (w-nw)//2\n",
        "        dy = (h-nh)//2\n",
        "        image_data=0\n",
        "        if proc_img:\n",
        "            image = image.resize((nw,nh), Image.BICUBIC)\n",
        "            new_image = Image.new('RGB', (w,h), (128,128,128))\n",
        "            new_image.paste(image, (dx, dy))\n",
        "            image_data = np.array(new_image)/255.\n",
        "\n",
        "        # correct boxes\n",
        "        box_data = np.zeros((max_boxes,5))\n",
        "        if len(box)>0:\n",
        "            np.random.shuffle(box)\n",
        "            if len(box)>max_boxes: box = box[:max_boxes]\n",
        "            box[:, [0,2]] = box[:, [0,2]]*scale + dx\n",
        "            box[:, [1,3]] = box[:, [1,3]]*scale + dy\n",
        "            box_data[:len(box)] = box\n",
        "\n",
        "        return image_data, box_data\n",
        "\n",
        "    # resize image\n",
        "    new_ar = w/h * rand(1-jitter,1+jitter)/rand(1-jitter,1+jitter)\n",
        "    scale = rand(.25, 2)\n",
        "    if new_ar < 1:\n",
        "        nh = int(scale*h)\n",
        "        nw = int(nh*new_ar)\n",
        "    else:\n",
        "        nw = int(scale*w)\n",
        "        nh = int(nw/new_ar)\n",
        "    image = image.resize((nw,nh), Image.BICUBIC)\n",
        "\n",
        "    # place image\n",
        "    dx = int(rand(0, w-nw))\n",
        "    dy = int(rand(0, h-nh))\n",
        "    new_image = Image.new('RGB', (w,h), (128,128,128))\n",
        "    new_image.paste(image, (dx, dy))\n",
        "    image = new_image\n",
        "\n",
        "    # flip image or not\n",
        "    flip = rand()<.5\n",
        "    if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "    # distort image\n",
        "    hue = rand(-hue, hue)\n",
        "    sat = rand(1, sat) if rand()<.5 else 1/rand(1, sat)\n",
        "    val = rand(1, val) if rand()<.5 else 1/rand(1, val)\n",
        "    x = rgb_to_hsv(np.array(image)/255.)\n",
        "    x[..., 0] += hue\n",
        "    x[..., 0][x[..., 0]>1] -= 1\n",
        "    x[..., 0][x[..., 0]<0] += 1\n",
        "    x[..., 1] *= sat\n",
        "    x[..., 2] *= val\n",
        "    x[x>1] = 1\n",
        "    x[x<0] = 0\n",
        "    image_data = hsv_to_rgb(x) # numpy array, 0 to 1\n",
        "\n",
        "    # correct boxes\n",
        "    box_data = np.zeros((max_boxes,5))\n",
        "    if len(box)>0:\n",
        "        np.random.shuffle(box)\n",
        "        box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n",
        "        box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n",
        "        if flip: box[:, [0,2]] = w - box[:, [2,0]]\n",
        "        box[:, 0:2][box[:, 0:2]<0] = 0\n",
        "        box[:, 2][box[:, 2]>w] = w\n",
        "        box[:, 3][box[:, 3]>h] = h\n",
        "        box_w = box[:, 2] - box[:, 0]\n",
        "        box_h = box[:, 3] - box[:, 1]\n",
        "        box = box[np.logical_and(box_w>1, box_h>1)] # discard invalid box\n",
        "        if len(box)>max_boxes: box = box[:max_boxes]\n",
        "        box_data[:len(box)] = box\n",
        "\n",
        "    return image_data, box_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_cq6RKl7vyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout_rate = 0.5\n",
        "alpha = 0.2\n",
        "\n",
        "def calculate_iou( target_boxes , pred_boxes ):\n",
        "    xA = K.maximum( target_boxes[ ... , 0], pred_boxes[ ... , 0] )\n",
        "    yA = K.maximum( target_boxes[ ... , 1], pred_boxes[ ... , 1] )\n",
        "    xB = K.minimum( target_boxes[ ... , 2], pred_boxes[ ... , 2] )\n",
        "    yB = K.minimum( target_boxes[ ... , 3], pred_boxes[ ... , 3] )\n",
        "    interArea = K.maximum( 0.0 , xB - xA ) * K.maximum( 0.0 , yB - yA )\n",
        "    boxAArea = (target_boxes[ ... , 2] - target_boxes[ ... , 0]) * (target_boxes[ ... , 3] - target_boxes[ ... , 1])\n",
        "    boxBArea = (pred_boxes[ ... , 2] - pred_boxes[ ... , 0]) * (pred_boxes[ ... , 3] - pred_boxes[ ... , 1])\n",
        "    iou = interArea / ( boxAArea + boxBArea - interArea )\n",
        "    return iou\n",
        "\n",
        "def custom_loss( y_true , y_pred ):\n",
        "    mse = tf.losses.mean_squared_error( y_true , y_pred ) \n",
        "    iou = calculate_iou( y_true , y_pred ) \n",
        "    return mse + ( 1 - iou )\n",
        "\n",
        "def iou_metric( y_true , y_pred ):\n",
        "    return calculate_iou( y_true , y_pred )\n",
        "\t\t\n",
        "class BoundBox:\n",
        "\tdef __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
        "\t\tself.xmin = xmin\n",
        "\t\tself.ymin = ymin\n",
        "\t\tself.xmax = xmax\n",
        "\t\tself.ymax = ymax\n",
        "\t\tself.objness = objness\n",
        "\t\tself.classes = classes\n",
        "\t\tself.label = -1\n",
        "\t\tself.score = -1\n",
        " \n",
        "\tdef get_label(self):\n",
        "\t\tif self.label == -1:\n",
        "\t\t\tself.label = np.argmax(self.classes)\n",
        " \n",
        "\t\treturn self.label\n",
        " \n",
        "\tdef get_score(self):\n",
        "\t\tif self.score == -1:\n",
        "\t\t\tself.score = self.classes[self.get_label()]\n",
        " \n",
        "\t\treturn self.score\n",
        " \n",
        "def _sigmoid(x):\n",
        "\treturn 1. / (1. + np.exp(-x))\n",
        " \n",
        "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
        "\tgrid_h, grid_w = netout.shape[:2]\n",
        "\tnb_box = 3\n",
        "\tnetout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
        "\tnb_class = netout.shape[-1] - 5\n",
        "\tboxes = []\n",
        "\tnetout[..., :2]  = _sigmoid(netout[..., :2])\n",
        "\tnetout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
        "\tnetout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
        "\tnetout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
        " \n",
        "\tfor i in range(grid_h*grid_w):\n",
        "\t\trow = i / grid_w\n",
        "\t\tcol = i % grid_w\n",
        "\t\tfor b in range(nb_box):\n",
        "\t\t\t# 4th element is objectness score\n",
        "\t\t\tobjectness = netout[int(row)][int(col)][b][4]\n",
        "\t\t\tif(objectness.all() <= obj_thresh): continue\n",
        "\t\t\t# first 4 elements are x, y, w, and h\n",
        "\t\t\tx, y, w, h = netout[int(row)][int(col)][b][:4]\n",
        "\t\t\tx = (col + x) / grid_w # center position, unit: image width\n",
        "\t\t\ty = (row + y) / grid_h # center position, unit: image height\n",
        "\t\t\tw = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
        "\t\t\th = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n",
        "\t\t\t# last elements are class probabilities\n",
        "\t\t\tclasses = netout[int(row)][col][b][5:]\n",
        "\t\t\tbox = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
        "\t\t\tboxes.append(box)\n",
        "\treturn boxes\n",
        " \n",
        "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
        "\tnew_w, new_h = net_w, net_h\n",
        "\tfor i in range(len(boxes)):\n",
        "\t\tx_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
        "\t\ty_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
        "\t\tboxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
        "\t\tboxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
        "\t\tboxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
        "\t\tboxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
        " \n",
        "def _interval_overlap(interval_a, interval_b):\n",
        "\tx1, x2 = interval_a\n",
        "\tx3, x4 = interval_b\n",
        "\tif x3 < x1:\n",
        "\t\tif x4 < x1:\n",
        "\t\t\treturn 0\n",
        "\t\telse:\n",
        "\t\t\treturn min(x2,x4) - x1\n",
        "\telse:\n",
        "\t\tif x2 < x3:\n",
        "\t\t\t return 0\n",
        "\t\telse:\n",
        "\t\t\treturn min(x2,x4) - x3\n",
        " \n",
        "def bbox_iou(box1, box2):\n",
        "\tintersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
        "\tintersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
        "\tintersect = intersect_w * intersect_h\n",
        "\tw1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
        "\tw2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
        "\tunion = w1*h1 + w2*h2 - intersect\n",
        "\treturn float(intersect) / union\n",
        " \n",
        "def do_nms(boxes, nms_thresh):\n",
        "\tif len(boxes) > 0:\n",
        "\t\tnb_class = len(boxes[0].classes)\n",
        "\telse:\n",
        "\t\treturn\n",
        "\tfor c in range(nb_class):\n",
        "\t\tsorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
        "\t\tfor i in range(len(sorted_indices)):\n",
        "\t\t\tindex_i = sorted_indices[i]\n",
        "\t\t\tif boxes[index_i].classes[c] == 0: continue\n",
        "\t\t\tfor j in range(i+1, len(sorted_indices)):\n",
        "\t\t\t\tindex_j = sorted_indices[j]\n",
        "\t\t\t\tif bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
        "\t\t\t\t\tboxes[index_j].classes[c] = 0\n",
        " \n",
        "def load_image_pixels(filename, shape):\n",
        "\timage = load_img(filename)\n",
        "\twidth, height = image.size\n",
        "\timage = load_img(filename, target_size=shape)\n",
        "\timage = img_to_array(image)\n",
        "\timage = image.astype('float32')\n",
        "\timage /= 255.0\n",
        "\timage = expand_dims(image, 0)\n",
        "\treturn image, width, height\n",
        " \n",
        "def get_boxes(boxes, labels, thresh):\n",
        "\tv_boxes, v_labels, v_scores = list(), list(), list()\n",
        "\tfor box in boxes:\n",
        "\t\tfor i in range(len(labels)):\n",
        "\t\t\tif box.classes[i] > thresh:\n",
        "\t\t\t\tv_boxes.append(box)\n",
        "\t\t\t\tv_labels.append(labels[i])\n",
        "\t\t\t\tv_scores.append(box.classes[i]*100)\n",
        "\treturn v_boxes, v_labels, v_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq45dprDFhQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_boxes(filename, v_boxes, v_labels, v_scores):\n",
        "\tdata = pyplot.imread(filename)\n",
        "\tpyplot.imshow(data)\n",
        "\tax = pyplot.gca()\n",
        "\tfor i in range(len(v_boxes)):\n",
        "\t\tbox = v_boxes[i]\n",
        "\t\ty1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
        "\t\twidth, height = x2 - x1, y2 - y1\n",
        "\t\trect = Rectangle((x1, y1), width, height, fill=False, color='white')\n",
        "\t\tax.add_patch(rect)\n",
        "\t\tlabel = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n",
        "\t\tpyplot.text(x1, y1, label, color='white')\n",
        "\tpyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHhITWBFh2Rf",
        "colab_type": "code",
        "outputId": "60bbc2f7-84ac-40b8-f422-acbaaacd4031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "#####SCRATCHBOOK########\n",
        "for voc_example in voc_train.take(10):\n",
        "  image= voc_example['image']\n",
        "  #image = np.reshape(image, (448,448,3))\n",
        "  label = voc_example['labels']\n",
        "  objects = voc_example['objects']\n",
        "  bbox = objects['bbox']\n",
        "  #x=tf.concat([image, x], 0)\n",
        "  x=tf.image.resize_with_crop_or_pad(image, 448, 448)\n",
        "  x=tf.reshape(x,(5,))\n",
        "  x=bbox.numpy()\n",
        "  print(x)\n",
        "  #print(label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.14375    0.0437018  0.97083336 0.7763496 ]\n",
            " [0.14583333 0.24164525 0.57916665 0.6066838 ]\n",
            " [0.6        0.5244216  0.8541667  0.76606685]\n",
            " [0.56041664 0.5012854  0.7395833  0.6863753 ]]\n",
            "[[0.07466666 0.08       0.88533336 0.812     ]\n",
            " [0.304      0.002      0.73866665 0.292     ]]\n",
            "[[0.264 0.006 1.    1.   ]]\n",
            "[[0.138      0.00533333 0.998      0.9813333 ]]\n",
            "[[0.3153153 0.314     0.8318318 0.932    ]]\n",
            "[[0.18933333 0.002      1.         0.352     ]\n",
            " [0.00533333 0.3        1.         1.        ]]\n",
            "[[0.424      0.208      0.986      0.84533334]\n",
            " [0.196      0.02933333 0.482      0.8746667 ]\n",
            " [0.272      0.21866667 0.756      0.85866666]]\n",
            "[[0.16533333 0.158      1.         1.        ]]\n",
            "[[0.59516615 0.194      0.7703928  0.286     ]\n",
            " [0.68882173 0.058      0.9848943  0.188     ]\n",
            " [0.5861027  0.658      0.7734139  0.75      ]\n",
            " [0.6676737  0.754      0.91540784 0.88      ]\n",
            " [0.5861027  0.104      0.978852   0.876     ]]\n",
            "[[0.2072072 0.028     0.6576577 0.974    ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AchRVHBKUAsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = createModel()\n",
        "batch_size = 256\n",
        "epochs = 50\n",
        "model1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model1.fit(X_train, Y_train)\n",
        "model1.evaluate(test_data, test_labels_one_hot)\n",
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLji1F1uEuTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##TODO: skip layers, residual blocks\n",
        "def createModel():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=(1,1), activation=\"relu\"))\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=(2,2), activation=\"relu\"))\n",
        "        \n",
        "    model.add(Conv2D(32, kernel_size=1, strides=(1,1), activation=\"relu\"))\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=(1,1), activation=\"relu\"))\n",
        "    \n",
        "    model.add(Conv2D(128, kernel_size=3, strides=(2,2), activation=\"relu\"))\n",
        "    \n",
        "    for _ in range(2):\n",
        "      model.add(Conv2D(64, kernel_size=3,  strides=(1,1), activation=\"relu\"))\n",
        "      model.add(Conv2D(128, kernel_size=3, strides=(1,1),activation=\"relu\"))\n",
        "    \n",
        "    model.add(Conv2D(256, kernel_size=3, strides=(2,2), activation=\"relu\"))\n",
        "    for _ in range(8):\n",
        "      model.add(Conv2D(128, kernel_size=3, strides=(1,1), activation=\"relu\"))\n",
        "      model.add(Conv2D(256, kernel_size=3, strides=(1,1), activation=\"relu\"))\n",
        "    \n",
        "    model.add(Conv2D(512, kernel_size=3, strides=(2,2), activation=\"relu\"))\n",
        "    for _ in range(8):\n",
        "      model.add(Conv2D(256, kernel_size=3, strides=(1,1), activation=\"relu\"))\n",
        "      model.add(Conv2D(512, kernel_size=3, strides=(1,1), activation=\"relu\"))\n",
        "    \n",
        "    model.add(Conv2D(1024, kernel_size=3, strides=(2,2), activation=\"relu\"))\n",
        "    for _ in range(4):\n",
        "      model.add(Conv2D(512, kernel_size=3, activation=\"relu\"))\n",
        "      model.add(Conv2D(1024, kernel_size=3, activation=\"relu\"))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "        \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYaBbvYkQPa7",
        "colab_type": "code",
        "outputId": "b3b7adc8-8db1-490c-f3e0-6b70b8c8ed96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        }
      },
      "source": [
        "voc_builder = tfds.builder(\"voc\")\n",
        "info = voc_builder.info\n",
        "print(info)\n",
        "print(info.features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='voc',\n",
            "    version=4.0.0,\n",
            "    description='This dataset contains the data from the PASCAL Visual Object Classes Challenge\n",
            "2007, a.k.a. VOC2007, corresponding to the Classification and Detection\n",
            "competitions.\n",
            "A total of 9963 images are included in this dataset, where each image\n",
            "contains a set of objects, out of 20 different classes, making a total of\n",
            "24640 annotated objects.\n",
            "In the Classification competition, the goal is to predict the set of labels\n",
            "contained in the image, while in the Detection competition the goal is to\n",
            "predict the bounding box and label of each individual object.\n",
            "WARNING: As per the official dataset, the test set of VOC2012 does not contain\n",
            "annotations.\n",
            "',\n",
            "    homepage='http://host.robots.ox.ac.uk/pascal/VOC/voc2007/',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
            "        'image/filename': Text(shape=(), dtype=tf.string),\n",
            "        'labels': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=20)),\n",
            "        'labels_no_difficult': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=20)),\n",
            "        'objects': Sequence({\n",
            "            'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),\n",
            "            'is_difficult': Tensor(shape=(), dtype=tf.bool),\n",
            "            'is_truncated': Tensor(shape=(), dtype=tf.bool),\n",
            "            'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),\n",
            "            'pose': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),\n",
            "        }),\n",
            "    }),\n",
            "    total_num_examples=9963,\n",
            "    splits={\n",
            "        'test': 4952,\n",
            "        'train': 2501,\n",
            "        'validation': 2510,\n",
            "    },\n",
            "    supervised_keys=None,\n",
            "    citation=\"\"\"@misc{pascal-voc-2007,\n",
            "    \tauthor = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n",
            "    \ttitle = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults\",\n",
            "    \thowpublished = \"http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\"}\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n",
            "FeaturesDict({\n",
            "    'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
            "    'image/filename': Text(shape=(), dtype=tf.string),\n",
            "    'labels': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=20)),\n",
            "    'labels_no_difficult': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=20)),\n",
            "    'objects': Sequence({\n",
            "        'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),\n",
            "        'is_difficult': Tensor(shape=(), dtype=tf.bool),\n",
            "        'is_truncated': Tensor(shape=(), dtype=tf.bool),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),\n",
            "        'pose': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),\n",
            "    }),\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlod_ENfMGm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Q9CZ_GTnyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_block(inputs, convs, skip=True):\n",
        "\tplaceholder = inputs\n",
        "\tcount = 0\n",
        "\tfor conv in convs:\n",
        "\t\tif count == (len(convs) - 2) and skip:\n",
        "\t\t\tskip_connection = placeholder\n",
        "\t\tcount += 1\n",
        "\t\tif conv['stride'] > 1: placeholder = ZeroPadding2D(((1,0),(1,0)))(placeholder) \n",
        "\t\tplaceholder = Conv2D(conv['filter'],\n",
        "\t\t\t\t   conv['kernel'],\n",
        "\t\t\t\t   strides=conv['stride'],\n",
        "\t\t\t\t   padding='valid' if conv['stride'] > 1 else 'same',\n",
        "\t\t\t\t   name='conv_' + str(conv['layer_idx']),\n",
        "\t\t\t\t   use_bias=False if conv['bnorm'] else True)(placeholder)\n",
        "\t\tif conv['bnorm']: placeholder = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(placeholder)\n",
        "\t\tif conv['leaky']: placeholder = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(placeholder)\n",
        "\treturn add([skip_connection, placeholder]) if skip else placeholder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt4tI_M9jMkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_yolov3_model(input_images):\n",
        "\tinput_image = Input(shape=(None, None, 3))\n",
        "\t# Layer  0 => 4\n",
        "\tx = conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
        "\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
        "\t\t\t\t\t\t\t\t  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
        "\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
        "\t# Layer  5 => 8\n",
        "\tx = conv_block(x, [{'filter': 128, 'kernel' : 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
        "\t\t\t\t\t\t\t\t\t\t\t{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
        "\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
        "\t# Layer  9 => 11\n",
        "\tx = conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
        "\t\t\t\t\t\t\t\t\t\t {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
        "\t# Layer 12 => 15\n",
        "\tx = conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
        "\t\t\t\t\t\t\t\t\t\t {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
        "\t\t\t\t\t\t\t\t\t\t {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
        "\t# Layer 16 => 36\n",
        "\tfor i in range(7):\n",
        "\t\tx = conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
        "\t\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
        "\tskip_36 = x\n",
        "\t# Layer 37 => 40\n",
        "\tx = conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
        "\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
        "\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
        "\t# Layer 41 => 61\n",
        "\tfor i in range(7):\n",
        "\t\tx = conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
        "\t\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
        "\tskip_61 = x\n",
        "\t# Layer 62 => 65\n",
        "\tx = conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
        "\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
        "\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
        "\t# Layer 66 => 74\n",
        "\tfor i in range(3):\n",
        "\t\tx = conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
        "\t\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
        "\t# Layer 75 => 79\n",
        "\tx = conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
        "\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
        "\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
        "\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
        "\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
        "\t# Layer 80 => 82\n",
        "\tyolo_82 = conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
        "\t\t\t\t\t\t\t  {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
        "\t# Layer 83 => 86\n",
        "\tx = conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
        "\tx = UpSampling2D(2)(x)\n",
        "\tx = concatenate([x, skip_61])\n",
        "\t# Layer 87 => 91\n",
        "\tx = conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
        "\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
        "\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
        "\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
        "\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
        "\t# Layer 92 => 94\n",
        "\tyolo_94 = conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
        "\t\t\t\t\t\t\t  {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
        "\t# Layer 95 => 98\n",
        "\tx = conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
        "\tx = UpSampling2D(2)(x)\n",
        "\tx = concatenate([x, skip_36])\n",
        "\t #Layer 99 => 106\n",
        "\tyolo_106 = conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
        "\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
        "\t\t\t\t\t\t\t   {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
        "\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
        "\t\t\t\t\t\t\t   {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
        "\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
        "\t\t\t\t\t\t\t   {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
        "\tmodel = Model(input_image, [yolo_82, yolo_94, yolo_106])\n",
        "\t#print(x)\n",
        "\t#print('model'+ str(model))\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf-akp1lMLB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_test, info = tfds.load(\"mnist\", split=\"test\", with_info=True)\n",
        "print(info)\n",
        "fig = tfds.show_examples(info, voc_test)\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sl97g-rrVod",
        "colab_type": "code",
        "outputId": "b423225a-4778-4475-cb19-4f92b2b97a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "#download mnist data and split into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNimpWsbrdjF",
        "colab_type": "code",
        "outputId": "e7a10164-b39c-438e-f448-ad38f3b1a835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plot the first image in the dataset\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7a42075ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRB2IAnCDiTx/65XcTNOWsh5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIQJ_hD4rhmj",
        "colab_type": "code",
        "outputId": "91c616b4-416e-4584-861f-5c533f99609f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBicAbUErj6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reshape data to fit model\n",
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPQAZeoDrl9m",
        "colab_type": "code",
        "outputId": "d202553b-42de-4a63-a0bd-91b4d15d475f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "#one-hot encode target column\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "y_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUfDfvzIrn_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "#create model\n",
        "model = Sequential()\n",
        "#add model layers\n",
        "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_Nrs05Nrra3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compile model using accuracy to measure model performance\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYnJhVq7rydR",
        "colab_type": "code",
        "outputId": "00c485e6-f9c3-4296-b7d7-c1afb0b2a99b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.2891 - accuracy: 0.9508 - val_loss: 0.0981 - val_accuracy: 0.9708\n",
            "Epoch 2/3\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0746 - accuracy: 0.9776 - val_loss: 0.0931 - val_accuracy: 0.9734\n",
            "Epoch 3/3\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0510 - accuracy: 0.9842 - val_loss: 0.0921 - val_accuracy: 0.9734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f7aa00f91d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8tiWzBesMpK",
        "colab_type": "code",
        "outputId": "424b7fef-5793-4511-88a3-08a109ed04fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWGngDOrr0WB",
        "colab_type": "code",
        "outputId": "c03cf1c4-406d-4a0a-8146-387dad0f081a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53KKgx6Fs9AQ",
        "colab_type": "code",
        "outputId": "199cfc39-6732-4150-87ae-71065aef34e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgbUKSE9tqx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "old_image = np.ones((100,100,3))\n",
        "new_image = np.ones((100,100,3,1))    \n",
        "\n",
        "# lets jsut say you have two Images \n",
        "old_image = np.reshape(old_image , (100,100,3,1))\n",
        "new_image = np.reshape(new_image , (100,100,3,1))\n",
        "directory = np.append( new_image , old_image , axis = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKQAg4Jzd_BT",
        "colab_type": "code",
        "outputId": "111538cd-6ab9-4410-f3f4-468efcd619b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "directory.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 100, 3, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty2T1KTyeE3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Conv2D(input_shape = (416,416,3), filters = 16, \n",
        "                         kernel_size = 3, \n",
        "                         strides = (1, 1),  \n",
        "                         padding = \"same\",\n",
        "                         data_format='channels_last')\n",
        "                 )\n",
        "model1.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
        "model1.add(Conv2D(filters = 32, \n",
        "                         kernel_size = 3, \n",
        "                         strides = (1, 1),\n",
        "                         padding = \"same\",\n",
        "                         data_format='channels_last')\n",
        "                 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78N5K_-zLNrm",
        "colab_type": "code",
        "outputId": "054928b5-6039-4788-e0ac-d7495e44f8a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_278 (Conv2D)          (None, 416, 416, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_73 (MaxPooling (None, 208, 208, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_279 (Conv2D)          (None, 208, 208, 32)      4640      \n",
            "=================================================================\n",
            "Total params: 5,088\n",
            "Trainable params: 5,088\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ9Y2SZkLlUV",
        "colab_type": "code",
        "outputId": "faf55c4f-ca51-496e-ff53-4f2c7b991b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "for year, image_set in sets:\n",
        "  image_ids = open('/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split()\n",
        "  print(image_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-c38760163e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mimage_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/ImageSets/Main/%s.txt'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: not all arguments converted during string formatting"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KdG34AlSOgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for file in glob.glob(\"*.txt\"):\n",
        "    fileTemp = file.split(\"_\")[1].split(\".txt\")[0]\n",
        "    if fileTemp == \"train\":\n",
        "      file_dirs= '/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/ImageSets/Main/' +str(file)\n",
        "      f = open(file_dirs, \"r+\")\n",
        "      line= f.read()\n",
        "      for l in range(len(line)):\n",
        "        year = line.split(\"_\")[0]\n",
        "        image_id = line.split(\"_\")[1].split(\" \")[0]\n",
        "        print(image_id)\n",
        "  \n",
        "  os.chdir('/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/JPEGImages/')\n",
        "  #for file in glob.glob(\"*.jpg\"):\n",
        "    #print(file)\n",
        "    #for image_id in image_ids:\n",
        "    #    list_file.write('/content/gdrive/My Drive/Colab Notebooks/sample_data/visDrone/JPEGImages/%s.jpg'%(wd, year, image_id))\n",
        "    #    convert_annotation(year, image_id, list_file)\n",
        "    #    list_file.write('\\n')\n",
        "    #list_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXcIAB73jKHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install certifi\n",
        "!pip install urllib3[secure]\n",
        "voc_train = tfds.load(name='voc', split='train')\n",
        "assert isinstance(voc_train, tf.data.Dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPffgCmMjXjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes= {}\n",
        "voc_builder = tfds.builder(\"voc\")\n",
        "info = voc_builder.info\n",
        "\n",
        "for i in range(len(info.features[\"labels\"].names)):\n",
        "  classes[i] = info.features[\"labels\"].names[i]\n",
        "\n",
        "len(classes)\n",
        "\n",
        "y_train = []\n",
        "X_train = []\n",
        "temp_labels = [0]*(len(classes)+1)\n",
        "\n",
        "for voc_example in voc_train.take(1):\n",
        "  image, label, objects = voc_example['image'], voc_example['labels'], voc_example['objects']\n",
        "  \n",
        "  image =tf.image.resize_with_crop_or_pad(image, 448, 448)\n",
        "  image = image.numpy()\n",
        "  image = np.asarray( image ) / 255.0\n",
        "  X_train=np.append( X_train , image)\n",
        "  \n",
        "  label = label.numpy()\n",
        "\n",
        "  bbox = objects['bbox']\n",
        "  bbox = bbox.numpy()  \n",
        "\n",
        "  #[[classscore],[x1],[y1],[x2],[y2], [one-hot encoding label]]\n",
        "  labels= np.append(label, 20)\n",
        "  labels = to_categorical(labels)\n",
        "  for k in range(len(labels)):\n",
        "    temp_labels = temp_labels + labels[k]\n",
        "  print(temp_labels)\n",
        "  for i in range(len(bbox[0])):\n",
        "    temp_array=[]\n",
        "    temp_array= np.append(temp_array, 1)\n",
        "    temp_array= np.append(temp_array, [bbox[i,0], bbox[i,1], bbox[i,2], bbox[i,3]]) \n",
        "    temp_array = np.append(temp_array, temp_labels[0:20])\n",
        "    y_train = np.append(y_train, temp_array)\n",
        "\n",
        "y_train = np.reshape(y_train,(4,25))\n",
        "y_train\n",
        "X_train= X_train.reshape((1,448, 448, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}